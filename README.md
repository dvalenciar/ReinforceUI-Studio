
<h1 align="center">
  <br>
ReinforceUI Studio
  <br>
</h1>

<p align="center">
  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/main.yml?label=CI&branch=main" alt="Build Status">
  </a>

  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/docker-publish.yml?label=Docker&branch=main" alt="Docker Status">
  </a>

  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions/workflows/formatting.yml">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/formatting.yml?label=Formatting&branch=main" alt="Formatting Status">
  </a>
  
  <a href="https://docs.reinforceui-studio.com/">
    <img  src="https://img.shields.io/badge/Docs-Up-blue" alt="Static Badge">
  </a>
  
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License">
  </a>
  <a href="https://www.python.org/downloads/release/python-310/">
    <img src="https://img.shields.io/badge/python-3.10-blue.svg" alt="Python Version">
  </a>

</p>

ReinforceUI Studio is a Python-based application designed to simplify the configuration and monitoring of Reinforcement Learning (RL) training processes. Featuring an intuitive graphical user interface (GUI), it eliminates the hassle of managing extra repositories or memorizing complex command lines.

Everything you need to train your RL model is provided in one repository. With just a few clicks, you can train your model, visualize the training process, and save the model for later useâ€”ready to be deployed and analyzed.


Please check out our **full documentation** available [here](https://docs.reinforceui-studio.com) for installation instructions, tutorials, RL concepts, and more.


<p align="center">
  <img src="media_resources/new_main_window_example.gif">
</p>

# Supported Algorithms
ReinforceUI Studio supports the following algorithms:

| Algorithm | Description |
| --- | --- |
| CTD4 | Continuous Distributional Actor-Critic Agent with a Kalman Fusion of Multiple Critics |
| DDPG | Deep Deterministic Policy Gradient |
| DQN | Deep Q-Network |
| PPO | Proximal Policy Optimization |
| SAC | Soft Actor-Critic |
| TD3 | Twin Delayed Deep Deterministic Policy Gradient |
| TQC | Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics |


# Why you should use ReinforceUI Studio
1. Simplified RL Workflows: The intuitive GUI eliminates the need for complex command-line operations
2. Environment Support: Seamlessly integrates with MuJoCo, OpenAI Gymnasium, and DeepMind Control Suite.
3. Algorithm Customization: Adjust hyperparameters and algorithms or use optimized defaults for quick experiments.
4. Real-Time Monitoring Dashboard: View training progress, metrics, and performance curves as they happen.
5. Comprehensive Data Logging: Automatically capture training data, evaluation results, plots, models, and videos for easy post-training analysis.
6. User-Friendly Training and Testing: Train, evaluate, and refine RL policies through a streamlined and intuitive workflow.

Check out our [video tutorial](https://www.youtube.com/watch?v=itXyyttwZ1M), where we show you how ReinforceUI Studio works


## Results Examples
Below are some examples of results generated by ReinforceUI Studio, showcasing the evaluation curves along with snapshots of the policies in action.

| **Algorithm** | **Platform** | **Environment**    | **Curve**                                                       | **Video**                                                                                        |
|---------------|--------------|--------------------|-----------------------------------------------------------------|--------------------------------------------------------------------------------------------------|
| **SAC**       | DMCS         | Walker Walk        | <img src="media_resources/result_examples/SAC_walker_walk.png" width="200">        | <img src="media_resources/result_examples/walker_walk.gif" width="200">       | 
| **TD3**       | MuJoCo       | HalfCheetah v5     | <img src="media_resources/result_examples/TD3_HalfCheetah-v5.png" width="200">     | <img src="media_resources/result_examples/HalfCheetah.gif" width="200">       |
| **CDT4**      | DMCS         | Ball in cup catch  | <img src="media_resources/result_examples/CTD4_ball_in_cup_catch.png" width="200"> | <img src="media_resources/result_examples/ball_in_cup_catch.gif" width="200"> | 
| **DQN**       | Gymnasium    | CartPole v1        | <img src="media_resources/result_examples/DQN_CartPole-v1.png" width="200">        | <img src="media_resources/result_examples/CartPole.gif" width="200">          | 


## Citation
If you find ReinforceUI Studio useful for your research or project, please kindly star this repo and cite is as follows:

```
@misc{reinforce_ui_studio_2025,
  title = { ReinforceUI Studio: Simplifying Reinforcement Learning Training and Monitoring},
  author = {David Valencia Redrovan},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/dvalenciar/ReinforceUI-Studio.}
}
```

## License
ReinforceUI Studio is licensed under the MIT License. You are free to use, modify, and distribute this software, 
provided that the original copyright notice and license are included in any copies or substantial portions of the software.


### Acknowledgements
This project was inspired by the CARES Reinforcement Learning Package from the University of Auckland 
