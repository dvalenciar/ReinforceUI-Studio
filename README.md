
<h1 align="center">
  <br>
ReinforceUI <Studio></Studio>
  <br>
</h1>

<p align="center">
  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/main.yml?label=CI&branch=main" alt="Build Status">
  </a>

  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/docker-publish.yml?label=Docker&branch=main" alt="Docker Status">
  </a>

  <a href="https://github.com/dvalenciar/ReinforceUI-Studio/actions/workflows/main.yml">
    <img src="https://img.shields.io/github/actions/workflow/status/dvalenciar/ReinforceUI-Studio/main.yml?label=Black&branch=main" alt="Black Status">
  </a>
  
  <a href="https://docs.reinforceui-studio.com/">
    <img  src="https://img.shields.io/badge/Docs-Up-blue" alt="Static Badge">
  </a>
  
  <a href="https://opensource.org/licenses/MIT">
    <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License">
  </a>
  <a href="https://www.python.org/downloads/release/python-310/">
    <img src="https://img.shields.io/badge/python-3.10-blue.svg" alt="Python Version">
  </a>

</p>

ReinforceUI Studio is a Python-based application designed to simplify the configuration and monitoring of Reinforcement Learning (RL) training processes. Featuring an intuitive graphical user interface (GUI), it eliminates the hassle of managing extra repositories or memorizing complex command lines.

Everything you need to train your RL model is provided in one repository. With just a few clicks, you can train your model, visualize the training process, and save the model for later use—ready to be deployed and analyzed.


Please check out our **full documentation** available [here](https://docs.reinforceui-studio.com) for installation instructions, tutorials, RL concepts, and more.


<p align="center">
  <img src="media_resources/main_window.gif">
</p>

# Why you should use ReinforceUI Studio
1. Simplified RL Workflows: The intuitive GUI eliminates the need for complex command-line operations
2. Environment Support: Seamlessly integrates with MuJoCo, OpenAI Gymnasium, and DeepMind Control Suite.
3. Algorithm Customization: Adjust hyperparameters and algorithms or use optimized defaults for quick experiments.
4. Real-Time Monitoring Dashboard: View training progress, metrics, and performance curves as they happen.
5. Comprehensive Data Logging: Automatically capture training data, evaluation results, plots, models, and videos for easy post-training analysis.
6. User-Friendly Training and Testing: Train, evaluate, and refine RL policies through a streamlined and intuitive workflow.

Check out our [video tutorial](https://www.youtube.com/watch?v=olaspgr3vlM), where we show you how ReinforceUI Studio works


## Contributing to ReinforceUI Studio
We welcome contributions to ReinforceUI Studio! Whether it’s bug fixes, feature suggestions, or documentation improvements, your input is valuable. Here’s how you can get involved:

1. Report Issues: Found a bug or have a suggestion? Open an issue in the repository, and we’ll take a look.
2. Fork the Repository: Clone the project and make your changes in a new branch.
3. Submit a Pull Request: Once your changes are ready, create a pull request with a clear description of your updates.

## Citation
If you find ReinforceUI Studio useful for your research or project, please kindly star this repo and cite is as follows:

```
@misc{reinforce_ui_studio_2025,
  title = { ReinforceUI Studio: Simplifying Reinforcement Learning Training and Monitoring},
  author = {David Valencia Redrovan},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/dvalenciar/ReinforceUI-Studio.}
}
```

## License
ReinforceUI Studio is licensed under the MIT License. You are free to use, modify, and distribute this software, 
provided that the original copyright notice and license are included in any copies or substantial portions of the software.


### Acknowledgements
This project was inspired by the CARES Reinforcement Learning Package from the University of Auckland 
